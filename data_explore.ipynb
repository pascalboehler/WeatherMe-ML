{
 "cells": [
  {
   "source": [
    "# Data explore Notebook\n",
    "\n",
    "In this notebook, I am going to explore the dataset to get to know it better and to extract the information I need to build a weather predicting ML Model\n",
    "\n",
    "But to get started analyzing the dataset, let's import the needed libraries. You can install them using the ```REQUIREMENTS.txt``` file in the GitHub Repository this notebook is located in."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "The dataset we are going to use for the project is a weather dataset found on kaggle.com on the 13th february 2021. You can find the dataset following [this link](https://www.kaggle.com/selfishgene/historical-hourly-weather-data).\n",
    "\n",
    "The data the dataset contains was collected between 2012 and 2017 and includes hourly weather data from 30 cities in the United States and Canada. Also the dataset includes the data from 6 cities in israel.\n",
    "\n",
    "The data the dataset includes are listed down here now:\n",
    "* humidity\n",
    "* pressure\n",
    "* temperature\n",
    "* wind_direction\n",
    "* wind_speed\n",
    "* weather_describtion\n",
    "* city_attributes\n",
    "\n",
    "All the different attributes are stored in multiple csv files, which are located inside the folder ./weather_data\n",
    "\n",
    "In the model, we are first focusing on predicting the temperature, by letting the model learn, which temperature it should expect for a given date, meaning that we will feed in the date as our feature and hopefully get a temperature value (label) in return. Later on, I might try to expand this project to incorparate more components that are necessary to predict the weather, but that will be seen by time (I will update this page when the rest of the project is changing).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing the files\n",
    "\n",
    "So now it is time for us to open up the csv dataset using pandas and display them in a table like form. Displaying it in this notebook in a table like form and plotting the data makes it easier to read than the comma seperated csv text files with thousands and thousands of lines of data. Also we won't display the complete dataset in a table, because the files have so many entries.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./weather_data/\"\n",
    "\n",
    "humidity = pd.read_csv(path + 'humidity.csv', index_col = 0)\n",
    "pressure = pd.read_csv(path + 'pressure.csv', index_col = 0)\n",
    "temperature = pd.read_csv(path + 'temperature.csv', index_col = 0)\n",
    "wind_direction = pd.read_csv(path + 'wind_direction.csv', index_col = 0)\n",
    "wind_speed = pd.read_csv(path + 'wind_speed.csv', index_col = 0)\n",
    "weather_description = pd.read_csv(path + 'weather_description.csv', index_col = 0)\n",
    "city_attributes = pd.read_csv(path + 'city_attributes.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tensorflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "252addc6114319e695edf3508f22670e6a132b125816fb910889a0ea120d90d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}