{
 "cells": [
  {
   "source": [
    "# Data explore Notebook\n",
    "\n",
    "In this notebook, I am going to explore the dataset to get to know it better and to extract the information I need to build a weather predicting ML Model\n",
    "\n",
    "But to get started analyzing the dataset, let's import the needed libraries. You can install them using the ```REQUIREMENTS.txt``` file in the GitHub Repository this notebook is located in."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn"
   ]
  },
  {
   "source": [
    "## Introduction\n",
    "\n",
    "The dataset we are going to use for the project is a weather dataset found on kaggle.com on the 13th February 2021. You can find the dataset following [this link](https://www.kaggle.com/selfishgene/historical-hourly-weather-data). The data is licensed under the [ODbL](https://opendatacommons.org/licenses/odbl/).\n",
    "\n",
    "The data the dataset contains was collected between 2012 and 2017 and includes hourly weather data from 30 cities in the United States and Canada. Also the dataset includes the data from 6 cities in Israel.\n",
    "\n",
    "The data the dataset includes are listed down here now:\n",
    "* humidity in %\n",
    "* pressure in hPa\n",
    "* temperature in Kelvin (K)\n",
    "* wind_direction in meteorological degress (°)\n",
    "* wind_speed in m/s\n",
    "* weather_describtion in text\n",
    "* city_attributes in text\n",
    "\n",
    "All the different attributes are stored in multiple csv files, which are located inside the folder ./weather_data\n",
    "\n",
    "In the model, we are first focusing on predicting the temperature, by letting the model learn, which temperature it should expect for a given date, meaning that we will feed in the date as our feature and hopefully get a temperature value (label) in return. Later on, I might try to expand this project to incorparate more components that are necessary to predict the weather, but that will be seen by time (I will update this page when the rest of the project is changing).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Importing the files\n",
    "\n",
    "So now it is time for us to open up the csv dataset using pandas DataFrames and display them in a table like form. Displaying it in this notebook in a table like form and plotting the data makes it easier to read than the comma seperated csv text files with thousands and thousands of lines of data. Also we won't display the complete dataset in a table, because the files have so many entries.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./weather_data_raw/\"\n",
    "\n",
    "humidity_raw = pd.read_csv(path + 'humidity.csv', index_col = 0)\n",
    "pressure_raw = pd.read_csv(path + 'pressure.csv', index_col = 0)\n",
    "temperature_raw = pd.read_csv(path + 'temperature.csv', index_col = 0)\n",
    "wind_direction_raw = pd.read_csv(path + 'wind_direction.csv', index_col = 0)\n",
    "wind_speed_raw = pd.read_csv(path + 'wind_speed.csv', index_col = 0)\n",
    "weather_description_raw = pd.read_csv(path + 'weather_description.csv', index_col = 0)\n",
    "city_attributes_raw = pd.read_csv(path + 'city_attributes.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Vancouver'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "cities = city_attributes_raw.index.values\n",
    "\n",
    "cities[0]"
   ]
  },
  {
   "source": [
    "## Splitting the data\n",
    "\n",
    "Each file we imported now, includes all the data for all the cities. But it would make our live easier if we split them up to a dictionary which includes one pandas DataFrame for each city so we can call them using the ```cityname``` key of our dictionary. Additionally we will rename the series titles from the city name to their actual name (temperature etc)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity = {}\n",
    "pressure = {}\n",
    "temperature = {}\n",
    "wind_direction = {}\n",
    "wind_speed = {}\n",
    "weather_description = {}\n",
    "city_attributes = {}\n",
    "\n",
    "for city in cities:\n",
    "    humidity[city] = pd.DataFrame(humidity_raw[city]).rename(columns={city: 'humidity'})\n",
    "    pressure[city] = pd.DataFrame(pressure_raw[city]).rename(columns={city: 'pressure'})\n",
    "    temperature[city] = pd.DataFrame(temperature_raw[city]).rename(columns={city: 'temperature'})\n",
    "    wind_direction[city] = pd.DataFrame(wind_direction_raw[city]).rename(columns={city: 'wind_direction'})\n",
    "    wind_speed[city] = pd.DataFrame(wind_speed_raw[city]).rename(columns={city: 'wind_speed'})\n",
    "    weather_description[city] = weather_description_raw[city]"
   ]
  },
  {
   "source": [
    "## Recombining the dataset\n",
    "\n",
    "To make it easier for us to train the model, we create new .csv files and panda DataFrames, where each file includes all the meteorological data for one specific city like Vancouver. This makes it easy to train the model only on one specific city first, which helps to reduce calculation times.\n",
    "\n",
    "The new csv file will have the following layout:\n",
    "\n",
    "Date | temperature | humidity | pressure | wind_speed | wind_direction\n",
    "------ | ------ | ------ | ------ | ------ | ------\n",
    "Unix timestamp | Temperature in Celcius | Humidity in % | Pressure in hPa | Wind speed in m/s | Wind direction in met degrees \n",
    "\n",
    "Because our dataset includes the temperature data in Kelvin and we want Celcius for our calculation, we will convert it using the following formula: ```Celcius [°C] = Kelvin [K] - 273.15```\n",
    "\n",
    "In addition to that, we will store all the DataFrames in a Python Dictionary, so we can access each city by it's name.\n",
    "\n",
    "To the disk, we will store the new csv files using the following naming system: *\\[city_name\\]_weather_dataset.csv*\n",
    "\n",
    "To uniform this, we also replace the spaces in our city name with -.\n",
    "\n",
    "Additionally, a json file including all the cities will be stored to the disk and called *cities.json*\n",
    "\n",
    "Knowing this, we can later access the data by importing the cities list and loading all the cities included in the list by opening the file with the following scheme."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data to celcius [°C]\n",
    "for city in cities:\n",
    "    for index, val in enumerate(temperature[city]['temperature']):\n",
    "        temperature[city]['temperature'][index] = val - 273.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for city in cities:\n",
    "    dataset[city] = pd.concat([temperature[city], humidity[city], pressure[city], wind_speed[city], wind_direction[city]], axis=1)"
   ]
  },
  {
   "source": [
    "## Writing to .csv file\n",
    "\n",
    "After we resorted the data into a more usable format, let's write the dataset to our disk. To do so, we run the following code below, which will store the data in this project directory in the weather_data folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for city in dataset:\n",
    "    filename = \"weather_data/\" + city.lower().replace(\" \", \"-\") + \"_weather_data_dataset.csv\"\n",
    "    dataset[city].to_csv(filename, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tensorflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "252addc6114319e695edf3508f22670e6a132b125816fb910889a0ea120d90d6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}